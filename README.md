# lm_training_benchmark

## Overview

This repo contains a rough draft of a benchmark for language model training. It is similar in spirit to the "gpt3" task in the [MLPerf Training benchmark](https://mlcommons.org/benchmarks/training/), but scaled down. The `nanogpt_model_lib.py` and `nanogpt_trainer_lib.py` files are adapted from commit 9755682 of [nanoGPT](https://github.com/karpathy/nanoGPT).

## Setup

The agent working on this task should be provided this codebase, the task description (see the "Task description" section below), a `data.bin` file, a `checkpoint0.bin` file and an `expected_checkpoint20.bin` file.

The `data.bin` file included in this repo is the `val.bin` file generated by following the instructions to prepare the openwebtext dataset in nanoGPT.

To generate the `checkpoint0.bin` file, run:

```bash
python train.py \
    --trainer="nanogpt" \
    --max_num_steps=0 \
    --seed=123 \
    --final_checkpoint_file="checkpoint0.bin"
```

To generate the `expected_checkpoint20.bin` file, run:

```bash
python train.py \
    --trainer="nanogpt" \
    --init_checkpoint_file="checkpoint0.bin" \
    --data_file="data.bin" \
    --max_num_steps=20 \
    --final_checkpoint_file="expected_checkpoint20.bin"
```

To prevent the agent from cheating, 2 sets of .bin files can be generated with the same formats and similar sizes. 1 set can be provided to the agent to help it experiment and the other set can be hidden from the agent and used for scoring.

To run tests:

```bash
pytest
```

## Task description

Modify `new_trainer_lib.py` to implement a trainer.

Your trainer should adhere to the API defined in `trainer_lib.py`.

You should NOT modify:
* any other code in this codebase
* the .bin files
* the name of `new_library_lib.py`
* the name of the Trainer class in `new_library_lib.py`

Your trainer will be scored by running:

```bash
python train.py \
    --trainer="new" \
    --init_checkpoint_file="checkpoint0.bin" \
    --expected_checkpoint_file="expected_checkpoint20.bin" \
    --data_file="data.bin" \
    --max_num_steps=20 \
    --score_file="score.txt"
```

The score is the time it takes for your trainer to perform 20 steps of training (excluding the initialization of the trainer). 

The model weights at the end of training should match the model weights in `expected_checkpoint20.bin`. Otherwise, an error is thrown and no score is generated.

The goal is to get the lowest score possible.

An example of a Trainer is provided in `nanogpt_trainer_lib.py`. Running the command above with "nanogpt" instead of "new" for the trainer on a "gpu_1x_a100_sxm4" instance from [Lambda Labs](https://lambdalabs.com) ("1x A100 (40 GB SXM4), 30 CPU cores, 205.4 GB RAM, 525.8 GB SSD") results in a mean score of 71.5772 with standard deviation 0.0436 over 5 trials. The loss achieved is 9.8402.
